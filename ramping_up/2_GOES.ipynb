{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.7.6 64-bit (conda)",
   "display_name": "Python 3.7.6 64-bit (conda)",
   "metadata": {
    "interpreter": {
     "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# GOES Satellite Data\n",
    "\n",
    "GOES, or Geostationary Operational Environmental Satellite, is a meteorological satellite operated by NOAA. There are two GOES positions, GOES-East, and GOES-West that image weather conditions over the continental U.S. and Eastern Pacific and Northern Atlantic basins. The current satellites in the east and west positions are GOES-16 and GOES-17, respectively. \n",
    "\n",
    "There is a lot of information available about their satellites, data, and file structure. When it comes to automated GOES data retrieval, as is the goal of this notebook, one thing that is important to note is that the *scan mode* [has changed](https://cimss.ssec.wisc.edu/satellite-blog/archives/32657), and this is likely not the only change over the years. Scan mode 3 was the original default, but in order to obtain more frequent full-disk images, scan mode 6 became the default in April 2019. This change is reflected in the file name, so I initially was unable to get more recent data. \n",
    "\n",
    "The data can be obtained from Amazon Web Services. To access the data, we can use Amazon S3 or Amazon \"Simple Storage Service\". Python has a package, `boto3`, which is the AWS software development kit that we can use to get the data. In order to access the data, we have to know how it is stored. The files are stored in netCDF format in two AWS buckets, `noaa-goes16` and `noaa-goes17`. They are then stored in folders of the format\n",
    "\n",
    "<center>{Product}/{Year}/{Day of Year}/{Hour}/{Filename}.</center>\n",
    "\n",
    "The product here will just be `ABI-L1b-RadF`. This is the full-disc radiance from Level-1b (L1b) data generated from Advanced Baseline Imager (ABI). More specifics about L1b products can be found in the [public user's guide](https://www.goes-r.gov/users/docs/PUG-L1b-vol3.pdf). The ABI has 16 bands, but we'll choose band 2. This band has a central wavelength of 0.64 $\\mu$m, corresponding to the visible, red band. The filename has a somewhat complicated structure. Here's an example from [NOAA's GOES on AWS readme](https://docs.opendata.aws/noaa-goes16/cics-readme.html):\n",
    "\n",
    "<center><pre>OR_ABI-L1b-RadF-M3C02_G16_s20171671145342_e20171671156109_c20171671156144.nc</pre></center>\n",
    "\n",
    "Each segment of the file name and its description are shown below:\n",
    "\n",
    "File Name Segment | Description\n",
    "--- | ---\n",
    "`OR` | Operational system real-time data\n",
    "`ABI` | ABI Sensor\n",
    "`L1b` | processing level\n",
    "`Rad` | radiances\n",
    "`F` | full disk\n",
    "`M3` | mode 3 (scan operation)\n",
    "`C02` | channel or band 02\n",
    "`G16` | satellite id for GOES-16\n",
    "`s20171671145342` | start of scan time\n",
    "`e20171671156109` | end of scan time\n",
    "`c20171671156144` | netCDF4 file creation time\n",
    "`.nc` | netCDF file extension\n",
    "\n",
    "With the [AWS client](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-windows.html#cliv2-windows-prereq), we can search for an image via a command like `aws s3 ls s3://noaa-goes16/ABI-L1b-RadF/2018/271/12/ --no-sign-request`. This lists all of the ABI-L1b-RadF products from GOES16 on the 271st day of 2018 at 1200Z. This is a quick way to explore available imagery, rather than iteratively running this code or something similar."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Import Modules\n",
    "\n",
    "Let's first import the modules we need to run the code."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import requests\n",
    "import netCDF4\n",
    "import boto3\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import datetime"
   ]
  },
  {
   "source": [
    "## Define Custom Functions\n",
    "\n",
    "We need to create a few custom functions. We need the following:\n",
    "* `day_of_year` - a function that takes a date and converts how many days since January 1 of that year have passed\n",
    "* `read_aws_creds` - a function that reads my AWS credentials downloaded directly from AWS\n",
    "* `get_s3_keys` - a function that lists all objects in a bucket that start with a prefix string"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_of_year(date):\n",
    "\n",
    "    '''\n",
    "    Take a datetime date and get the number of days since Jan 1 of that same year\n",
    "    '''\n",
    "    \n",
    "    year = date.year\n",
    "    firstDay = datetime.datetime(year,1,1)\n",
    "    return (date-firstDay).days+1\n",
    "\n",
    "def read_aws_creds():\n",
    "\n",
    "    '''\n",
    "    Read AWS credentials stored at ~/rootkey.csv\n",
    "    '''\n",
    "\n",
    "    home = os.path.expanduser('~')\n",
    "    awsCredPath = home+'/rootkey.csv'\n",
    "    with open(awsCredPath,'r') as f:\n",
    "        creds = f.read()\n",
    "\n",
    "    return [cred.split('=')[1] for cred in creds.split('\\n')]\n",
    "\n",
    "def get_s3_keys(bucket, s3Client, prefix = ''):\n",
    "\n",
    "    \"\"\"\n",
    "    Generate the keys in an S3 bucket.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build arguments dictionary\n",
    "    kwargs = {'Bucket': bucket}\n",
    "    if isinstance(prefix, str):\n",
    "        kwargs['Prefix'] = prefix\n",
    "\n",
    "    while True:\n",
    "\n",
    "        resp = s3Client.list_objects_v2(**kwargs)\n",
    "        for obj in resp['Contents']:\n",
    "            key = obj['Key']\n",
    "            if key.startswith(prefix):\n",
    "                yield key\n",
    "\n",
    "        try:\n",
    "            kwargs['ContinuationToken'] = resp['NextContinuationToken']\n",
    "        except KeyError:\n",
    "            break"
   ]
  },
  {
   "source": [
    "## Set Image Parameters\n",
    "\n",
    "Now we set the parameters specifying the image and data we want. Let's set the date for the image to be 30 days ago from today at time 1800Z. Additionally, I want to see GOES-16 and GOES-17 around the same time, just to compare the two, so let's define both bucket names."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set image specific parameters\n",
    "bucketNameEast = 'noaa-goes16'\n",
    "bucketNameWest = 'noaa-goes17'\n",
    "productName = 'ABI-L1b-RadF'\n",
    "band = 2\n",
    "\n",
    "# Set date of image\n",
    "date = datetime.datetime.now()-datetime.timedelta(days=30)\n",
    "year = date.year\n",
    "day = day_of_year(date)\n",
    "hour = 18\n",
    "\n",
    "# Identify scan mode based on satellite/date\n",
    "if date < datetime.datetime(2019,4,2,16):\n",
    "    scanMode = \"M3\"\n",
    "else:\n",
    "    scanMode = \"M6\""
   ]
  },
  {
   "source": [
    "## Fetch Images from AWS\n",
    "\n",
    "Now we need to initialize the S3 client with our credentials. Then, we set the file name prefix for the parameters described above and query the bucket for any objects that begins with our file name prefix. Since the ABI images multiple times per hour, there will be several options, but we'll just grab the first image available for each hour."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize S3 client with credentials\n",
    "keyID,key = read_aws_creds()\n",
    "s3Client = boto3.client('s3',aws_access_key_id=keyID,aws_secret_access_key=key)\n",
    "\n",
    "# Set the file prefix string\n",
    "prefix = f'{productName}/{year}/{day:03.0f}/{hour:02.0f}/OR_{productName}-{scanMode}C{band:02.0f}'\n",
    "\n",
    "# Get the keys from the S3 bucket\n",
    "keysEast = get_s3_keys(bucketNameEast,s3Client,prefix)\n",
    "keysWest = get_s3_keys(bucketNameWest,s3Client,prefix)\n",
    "\n",
    "# Selecting the first measurement taken within the hour\n",
    "keyEast = [keyEast for keyEast in keysEast][0] \n",
    "keyWest = [keyWest for keyWest in keysWest][0] \n",
    "\n",
    "# Send a request to the bucket\n",
    "respEast = requests.get(f'https://{bucketNameEast}.s3.amazonaws.com/{keyEast}')\n",
    "respWest = requests.get(f'https://{bucketNameWest}.s3.amazonaws.com/{keyWest}')"
   ]
  },
  {
   "source": [
    "## Load the NetCDF File\n",
    "\n",
    "Now we use netCDF to load the file from the AWS request."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the GOES 16 image\n",
    "fileNameEast = keyEast.split('/')[-1].split('.')[0]\n",
    "nc4East = netCDF4.Dataset(fileNameEast,memory=respEast.content)\n",
    "storeEast = xr.backends.NetCDF4DataStore(nc4East)\n",
    "dsEast = xr.open_dataset(storeEast)\n",
    "\n",
    "# Open the GOES 17 image\n",
    "fileNameWest = keyWest.split('/')[-1].split('.')[0]\n",
    "nc4West = netCDF4.Dataset(fileNameWest,memory=respWest.content)\n",
    "storeWest = xr.backends.NetCDF4DataStore(nc4West)\n",
    "dsWest = xr.open_dataset(storeWest)"
   ]
  },
  {
   "source": [
    "## Plot the Image\n",
    "\n",
    "Lastly, let's plot our resulting image two images."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(30,15));\n",
    "\n",
    "# Fill plot data\n",
    "ax1.imshow(dsWest.Rad, cmap='gray');\n",
    "ax2.imshow(dsEast.Rad, cmap='gray');\n",
    "\n",
    "# Add titles\n",
    "fig.suptitle(date.strftime(\"%m/%d/%Y\")+\" at \"+str(hour)+\"Z\", fontsize=24);\n",
    "ax1.set_title('GOES-17, West',fontsize=18);\n",
    "ax2.set_title('GOES-16, East',fontsize=18);\n",
    "\n",
    "# Turn off axes\n",
    "ax1.axis('off');\n",
    "ax2.axis('off');\n",
    "\n",
    "# Save figure\n",
    "figFile = \"GOES_East-West_\"+date.strftime(\"%m%d%Y\"+str(hour)+\"Z\")\n",
    "plt.savefig(f'./images/{figFile}.png', dpi=300, facecolor='w', edgecolor='w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'date' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a975c295ddea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'date' is not defined"
     ]
    }
   ],
   "source": [
    "datezz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz"
   ]
  }
 ]
}