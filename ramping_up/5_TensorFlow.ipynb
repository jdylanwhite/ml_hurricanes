{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "victorian-gilbert",
   "metadata": {},
   "source": [
    "# Using TensorFlow\n",
    "\n",
    "The tutorials I went though in my Coursera courses used an outdated version of TensorFlow, so I need to relearn it all from first principles. It turns out that *a lot* has changed. The documentation for TensorFlow is quite good, and [there are several guides available](https://www.tensorflow.org/guide/). I'll go through some basics from the guides and then try using `tf.keras` on the MNIST dataset.\n",
    "\n",
    "First, we start with importing TensorFlow and other needed modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "interesting-cabin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import timeit\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reflected-arnold",
   "metadata": {},
   "source": [
    "Let's start with some simple variables and see how tensors work in TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pleased-plant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=() dtype=int32, numpy=5>\n",
      "<tf.Variable 'b:0' shape=() dtype=int32, numpy=2>\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Create variables\n",
    "a = tf.Variable(5)\n",
    "print(a)\n",
    "b = tf.Variable(2, name='b')\n",
    "print(b)\n",
    "print(b.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "separated-electronics",
   "metadata": {},
   "source": [
    "So the `Variable` is a stored TensorFlow quantity that can be changed, as opposed to `tf.constant` which cannot be updated. This is useful for things like our weights and biases which will update through backpropagation.  Additionally, we can call `.numpy` on a tensor to convert it to a `numpy` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "republican-maryland",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[1 2]\n",
      " [3 4]], shape=(2, 2), dtype=int32)\n",
      "\n",
      "[[1 2]\n",
      " [3 4]]\n"
     ]
    }
   ],
   "source": [
    "c = tf.constant([[1, 2],\n",
    "                 [3, 4]])\n",
    "print(c)\n",
    "print()\n",
    "print(c.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-calendar",
   "metadata": {},
   "source": [
    "We can also use broadcasting in TensorFlow. This will help a lot when adding weights and biases in a model which tend to be of size $n \\times m$ and $1 \\times m$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "independent-proposal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ 9 10]\n",
      " [11 12]], shape=(2, 2), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print(c+8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-director",
   "metadata": {},
   "source": [
    "Now let's have a look at functions. Efficient use of TensorFlow applies [graphs](https://www.tensorflow.org/guide/intro_to_graphs) for operations. This means that rather than calling `tensor1 + tensor2`, it's better to make these functions into a graph. This allows for faster computations, especially with serveral operations wrapped into a graph. According to the documentation,\n",
    "\n",
    "> \"Graphs are data structures that contain a set of tf.Operation objects, which represent units of computation; and `tf.Tensor` objects, which represent the units of data that flow between operations. They are defined in a `tf.Graph` context. Since these graphs are data structures, they can be saved, run, and restored all without the original Python code.\"\n",
    "\n",
    "We can create functions with a function decorator, where a function definition is preceded by `@tf.function`, or we can create a TensorFlow function object using `tf.function(myFunction)`. Additionally, `tf.function` recursively traces any function it calls, so if there are nested functions, we need only specify the outermost function as a `tf.function` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "geographic-assessment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(7, shape=(), dtype=int32)\n",
      "tf.Tensor(10, shape=(), dtype=int32)\n",
      "tf.Tensor(49, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# Create function with decorator\n",
    "@tf.function\n",
    "def add(a, b):\n",
    "    return a + b\n",
    "\n",
    "# Create function with tf.function\n",
    "def mult(a, b):\n",
    "    return a*b\n",
    "multFunction = tf.function(mult)\n",
    "\n",
    "# Create an inner and outer function, where only the outer function is a tf.function object\n",
    "def inner_function(a,b):\n",
    "    return (a+b)\n",
    "@tf.function\n",
    "def outer_function(a,b):\n",
    "    return inner_function(a,b)**2\n",
    "\n",
    "# Evaluate functions\n",
    "print(add(a,b))\n",
    "print(multFunction(a,b))\n",
    "print(outer_function(a,b))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "renewable-notification",
   "metadata": {},
   "source": [
    "The increased performance isn't really going to be seen with simple functions that simply add two numbers, but it can have a pretty big difference for substantially complex operations. Let's see the actual difference with a simple model that is wrapped in `tf.function` and another that is not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "macro-netherlands",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager time: 1.7726695000000001\n",
      "Graph time: 0.2920102999999994\n"
     ]
    }
   ],
   "source": [
    "# Create an oveerride model to classify pictures\n",
    "class SequentialModel(tf.keras.Model):\n",
    "  def __init__(self, **kwargs):\n",
    "    super(SequentialModel, self).__init__(**kwargs)\n",
    "    self.flatten = tf.keras.layers.Flatten(input_shape=(28, 28))\n",
    "    # Add a lot of small layers\n",
    "    num_layers = 100\n",
    "    self.my_layers = [tf.keras.layers.Dense(64, activation=\"relu\")\n",
    "                      for n in range(num_layers)]\n",
    "    self.dropout = tf.keras.layers.Dropout(0.2)\n",
    "    self.dense_2 = tf.keras.layers.Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.flatten(x)\n",
    "    for layer in self.my_layers:\n",
    "      x = layer(x)\n",
    "    x = self.dropout(x)\n",
    "    x = self.dense_2(x)\n",
    "    return x\n",
    "\n",
    "# Set input data\n",
    "input_data = tf.random.uniform([20, 28, 28])\n",
    "\n",
    "# Build the model without the tf.function specificaiton\n",
    "eager_model = SequentialModel()\n",
    "\n",
    "# Don't count the time for the initial build.\n",
    "eager_model(input_data)\n",
    "print(\"Eager time:\", timeit.timeit(lambda: eager_model(input_data), number=100))\n",
    "\n",
    "# Wrap the call method in a `tf.function`\n",
    "graph_model = SequentialModel()\n",
    "graph_model.call = tf.function(graph_model.call)\n",
    "\n",
    "# Don't count the time for the initial build and trace.\n",
    "graph_model(input_data)\n",
    "print(\"Graph time:\", timeit.timeit(lambda: graph_model(input_data), number=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-enzyme",
   "metadata": {},
   "source": [
    "So we can see a pretty notable speed up here in this simple model with 100 layers using graph computation rather than the default eager computations.\n",
    "\n",
    "## MNIST Classification\n",
    "\n",
    "Now let's use `tf.keras` to create a model to classify images in the MNIST dataset. This is taken almost directly from a tutorial on the TensorFlow documentation pages, [found here](https://www.tensorflow.org/tutorials/quickstart/beginner).\n",
    "\n",
    "Let's first load the dataset embedded within the TensorFlow package. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "casual-watershed",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-freight",
   "metadata": {},
   "source": [
    "Now we'll build our model by stacking layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "russian-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dropout(0.2),\n",
    "  tf.keras.layers.Dense(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tested-handy",
   "metadata": {},
   "source": [
    "The model will return a vector of \"logits\" or \"log-odds\" scores, one for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cardiac-banks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00139627, -0.13016906,  0.02925561, -0.15676871, -0.4639399 ,\n",
       "         0.14254001,  0.23802276,  0.5029071 ,  0.4270174 ,  0.4075299 ]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model(x_train[:1]).numpy()\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-matrix",
   "metadata": {},
   "source": [
    "We can then use SoftMax to convert these logits to probabilities for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "linear-mention",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.08682629, 0.07633538, 0.08952887, 0.07433165, 0.05467276,\n",
       "        0.10026789, 0.11031372, 0.1437697 , 0.13326278, 0.13069096]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.nn.softmax(predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fundamental-intensity",
   "metadata": {},
   "source": [
    "The `losses.SparseCategoricalCrossentropy` loss takes a vector of logits and a True index and returns a scalar loss for each example. This loss is equal to the negative log probability of the true class: It is zero if the model is sure of the correct class. This untrained model gives probabilities close to random (1/10 for each class), so the initial loss should be close to `-tf.log(1/10)` ~ 2.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dress-paragraph",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.2999098"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn(y_train[:1], predictions).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rapid-bouquet",
   "metadata": {},
   "source": [
    "Now we compile the model with specified a loss function and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "entire-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss=loss_fn,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-grade",
   "metadata": {},
   "source": [
    "Finally, running `model.fit` will train the model to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "limited-surveillance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.2940 - accuracy: 0.9154\n",
      "Epoch 2/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.1470 - accuracy: 0.9560\n",
      "Epoch 3/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.1078 - accuracy: 0.9671\n",
      "Epoch 4/5\n",
      "1875/1875 [==============================] - 3s 2ms/step - loss: 0.0884 - accuracy: 0.9729\n",
      "Epoch 5/5\n",
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0751 - accuracy: 0.9762\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x17b01e4e0a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-trunk",
   "metadata": {},
   "source": [
    "Now we can evaluate the accuracy of the model on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "consolidated-shopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 0.0745 - accuracy: 0.9767\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07445415109395981, 0.9767000079154968]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,  y_test, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funky-discovery",
   "metadata": {},
   "source": [
    "It looks like the model did well, at about 98\\% on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caroline-owner",
   "metadata": {},
   "source": [
    "## MNIST Classification with CNN\n",
    "\n",
    "Now that we've crated a model using a simple neural network, let's try using a CNN. This is taken almost directly from a tutorial on the TensorFlow documentation pages, [found here](https://www.tensorflow.org/tutorials/quickstart/advanced).\n",
    "\n",
    "We'll again start by loading the MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "olive-wheel",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "# Add a channels dimension\n",
    "x_train = x_train[..., tf.newaxis].astype(\"float32\")\n",
    "x_test = x_test[..., tf.newaxis].astype(\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interstate-sauce",
   "metadata": {},
   "source": [
    "Now we'll use `tf.data` to batch and shuffle the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "unauthorized-unemployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train, y_train)).shuffle(10000).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-congress",
   "metadata": {},
   "source": [
    "Now we build the `tf.keras` model using the Keras model subclassing API. Additionally, we'll choose an optimizer and loss function for training and select metrics to measure the loss and the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "proprietary-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    \n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.conv1 = Conv2D(32, 3, activation='relu')\n",
    "    self.flatten = Flatten()\n",
    "    self.d1 = Dense(128, activation='relu')\n",
    "    self.d2 = Dense(10)\n",
    "\n",
    "  def call(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.flatten(x)\n",
    "    x = self.d1(x)\n",
    "    return self.d2(x)\n",
    "\n",
    "# Create an instance of the model\n",
    "model = MyModel()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "# Define the metrics for loss and accuracy\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "greater-symbol",
   "metadata": {},
   "source": [
    "To train the model, we use `tf.GradientTape`. According to the documentation:\n",
    "\n",
    "> \"TensorFlow provides the `tf.GradientTape` API for automatic differentiation; that is, computing the gradient of a computation with respect to some inputs, usually `tf.Variable`s. TensorFlow 'records' relevant operations executed inside the context of a `tf.GradientTape` onto a \"tape\". TensorFlow then uses that tape to compute the gradients of a 'recorded' computation using reverse mode differentiation.\"\n",
    "\n",
    "So now let's create functions for training and testing the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "thorough-documentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    \n",
    "    with tf.GradientTape() as tape:    \n",
    "        # training=True is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n",
    "        predictions = model(images, training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(labels, predictions)\n",
    "    \n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    \n",
    "    # training=False is only needed if there are layers with different behavior during training versus inference (e.g. Dropout).\n",
    "    predictions = model(images, training=False)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "    \n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-senator",
   "metadata": {},
   "source": [
    "Now we're set to run the model and see the results. Let's run the model for 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "informative-marathon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.1328735500574112, Accuracy: 96.01667022705078, Test Loss: 0.0704181119799614, Test Accuracy: 97.73999786376953\n",
      "Epoch 2, Loss: 0.04222521185874939, Accuracy: 98.69499969482422, Test Loss: 0.05747160688042641, Test Accuracy: 98.12999725341797\n",
      "Epoch 3, Loss: 0.02281898260116577, Accuracy: 99.25333404541016, Test Loss: 0.05551176890730858, Test Accuracy: 98.25\n",
      "Epoch 4, Loss: 0.013914735987782478, Accuracy: 99.54833221435547, Test Loss: 0.06104082986712456, Test Accuracy: 98.18999481201172\n",
      "Epoch 5, Loss: 0.010572392493486404, Accuracy: 99.65666198730469, Test Loss: 0.0683385357260704, Test Accuracy: 98.13999938964844\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Reset the metrics at the start of the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()\n",
    "\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, '\n",
    "          f'Loss: {train_loss.result()}, '\n",
    "          f'Accuracy: {train_accuracy.result() * 100}, '\n",
    "          f'Test Loss: {test_loss.result()}, '\n",
    "          f'Test Accuracy: {test_accuracy.result() * 100}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-somewhere",
   "metadata": {},
   "source": [
    "And we have our image classifier trained to about 98\\% now. Using Keras in TensorFlow seems pretty easy, but this is certainly going to take some getting used to."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf)",
   "language": "python",
   "name": "tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
